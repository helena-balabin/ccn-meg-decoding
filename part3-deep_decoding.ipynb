{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdDUQwc5QhrH"
      },
      "source": [
        "# Part 3: Decoding with deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01KvvndTF2Zm"
      },
      "source": [
        "In this third part, we will build upon previous sections to construct a flexible deep learning pipeline to decode word embeddings from MEG data.\n",
        "\n",
        "**Contents**:\n",
        "\n",
        "0. Why decoding with deep learning?\n",
        "1. Preparing the dataloaders\n",
        "2. Building the deep learning model\n",
        "3. Designing the training loop\n",
        "4. Training the model\n",
        "5. Evaluating test performance\n",
        "6. Leveraging `exca` for large-scale experimentation\n",
        "7. Going further"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZKvRi4uWTjJ"
      },
      "source": [
        "| The tutorial assumes some prior knowledge of deep learning concepts:|\n",
        "| --- |\n",
        "|* Standard neural network architectures, e.g. convolutional neural networks|\n",
        "|* Optimization by batch gradient descent and backpropagation|\n",
        "|* Overfitting, early stopping, regularization|\n",
        "|* Some knowledge of pytorch and, optionally, of the pytorch lightning framework|\n",
        "*NOTE: You will still be able to run the whole notebook at your own pace and learn about these concepts along the way*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bDfN7KATdCQ"
      },
      "source": [
        "⚠️ **Before starting, make sure you're on a GPU instance for faster training!** ⚠️\n",
        "\n",
        "> If running on Google Colab, please request a GPU runtime by clicking `Runtime/Change runtime type` in the top bar menu, then selecting 'T4 GPU' under 'Hardware accelerator'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZK2rYtCSgEl",
        "outputId": "9fd6f607-4636-4ac9-80b2-26e34d4a86c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "replace data/LibriBrain2025_subject-0_session-1_run-1_task-Sherlock1_preproc.fif? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip -q /content/drive/MyDrive/data/libribrain2025_data.zip -d data\n",
        "\n",
        "# Identify whether a CUDA-enabled GPU is available\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cuda\":\n",
        "    msg ='CUDA-enabled GPU found. Training should be faster.'\n",
        "else:\n",
        "    msg = (\n",
        "        \"No GPU found. Training will be carried out on CPU, which might be \"\n",
        "        \"slower.\\n\\nIf running on Google Colab, you can request a GPU runtime by\"\n",
        "        \" clicking\\n`Runtime/Change runtime type` in the top bar menu, then \"\n",
        "        \"selecting \\'T4 GPU\\'\\nunder \\'Hardware accelerator\\'.\"\n",
        "    )\n",
        "print(msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HDaKnwhTFSx"
      },
      "outputs": [],
      "source": [
        "#@title ▶️ Let's download the data in case it's not yet downloaded\n",
        "\n",
        "if not Path(\"data\").exists():\n",
        "  !gdown 1jkadTwM2FbAojuwjRw_KdMpDoni7l4Xz -O data.zip\n",
        "  !unzip -q data.zip -d data\n",
        "\n",
        "!ls data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SadqlJ02D3Lo"
      },
      "outputs": [],
      "source": [
        "#@title ▶️ Install additional required packages for colab\n",
        "!pip install lightning==2.5.2 --no-deps  # to avoid having to install another torch version than the default one on colab\n",
        "!pip install pytorch-lightning --no-deps\n",
        "!pip install torchmetrics --no-deps\n",
        "!pip install lightning-utilities\n",
        "!pip install torchinfo\n",
        "!pip install exca\n",
        "!pip install mne\n",
        "!pip install spacy\n",
        "!pip install wordfreq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-Vnj2BkQhrK"
      },
      "source": [
        "## 0. Why decoding with deep learning?\n",
        "\n",
        "Linear models (Part 1) are fast to train, easy to interpret, and yield good performance. Why should we look into more complicated deep learning models?\n",
        "\n",
        "Some reasons include:\n",
        "\n",
        "1. Enables learning powerful **non-linear features end-to-end** -> often yields better decoding performance\n",
        "2. Facilitates **cross-subject learning** by using subject-specific layers or embeddings\n",
        "3. Allows using **custom learning objectives** such as contrastive losses (e.g. CLIP) to align predictions and modality (word) embeddings\n",
        "\n",
        "However, some caveats include:\n",
        "\n",
        "1. More computationally demanding\n",
        "2. Requires more data\n",
        "3. Requires tuning more hyperparameters\n",
        "\n",
        "Successful applications of deep models to M/EEG decoding tasks from our team include:\n",
        "* 💬 [Speech decoding (Défossez et al., 2023)](https://www.nature.com/articles/s42256-023-00714-5)\n",
        "* 🖼️ [Image decoding (Benchetrit et al., 2023)](https://arxiv.org/abs/2310.19812)\n",
        "* 📄 [Word decoding (d'Ascoli et al., 2024)](https://arxiv.org/abs/2412.17829)\n",
        "* ⌨️ [Typing decoding (Lévy et al., 2025)](https://arxiv.org/abs/2502.17480)\n",
        "\n",
        "![decoding](https://github.com/lucyzmf/NeuralDecoding-CCN2025/blob/main/images/image_decoding_linear_vs_deep.png?raw=1)\n",
        "*Per-subject performance of linear vs. deep models on an **image** decoding task, from [Banville et al., (2024)](https://arxiv.org/abs/2501.15322).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMzf8M0GQhrK"
      },
      "source": [
        "### In this notebook\n",
        "\n",
        "Below, we will build everything we need to train a deep learning model on a word embedding decoding task, including:\n",
        "\n",
        "- Preparing the dataloaders\n",
        "- Building the deep learning model\n",
        "- Designing the training loop\n",
        "- Training the model\n",
        "- Visualizing the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD9B9MWEXVQ0"
      },
      "outputs": [],
      "source": [
        "# Set some paths to locate the data and cache/save results\n",
        "DATA_DIR = Path(\"data\")\n",
        "CACHE_DIR = Path(\"cache\")\n",
        "RESULTS_DIR = Path(\"results\")\n",
        "\n",
        "assert DATA_DIR.exists()\n",
        "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4BDPcbeMQhrL"
      },
      "outputs": [],
      "source": [
        "#@title ▶️ Run this to redefine utilities from parts 1 and 2\n",
        "\n",
        "import typing as tp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import mne\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydantic\n",
        "import spacy\n",
        "from exca import MapInfra, TaskInfra\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from wordfreq import zipf_frequency\n",
        "\n",
        "\n",
        "class Neuro(pydantic.BaseModel):\n",
        "\n",
        "    model_config = pydantic.ConfigDict(extra=\"forbid\")\n",
        "\n",
        "    preproc_path: Path\n",
        "    fmin: float = 0.1\n",
        "    fmax: float = 40.\n",
        "    freq: float = 80.\n",
        "\n",
        "    apply_robust_scaler: bool = True  # whether to apply robust scaling to the neural data\n",
        "    tmin: float = -.5\n",
        "    tmax: float = 1.\n",
        "\n",
        "    def prepare_neuro(self, session: str) -> mne.io.Raw:\n",
        "\n",
        "        \"\"\" Load the raw neuro data and filter \"\"\"\n",
        "\n",
        "        file = Path(self.preproc_path) / f\"{session}_preproc.fif\"\n",
        "\n",
        "        if file.exists():\n",
        "            raw = mne.io.read_raw(file, verbose=\"ERROR\")\n",
        "\n",
        "        else:\n",
        "            fmin = self.fmin\n",
        "            fmax = self.fmax\n",
        "            freq = self.freq\n",
        "\n",
        "            original_file = self.preproc_path / f\"{session}.fif\" # original file\n",
        "            raw = mne.io.read_raw(original_file)\n",
        "            raw = raw.pick(picks=[\"meg\"]) # don't want to analyse misc\n",
        "\n",
        "            # band pass filter\n",
        "            raw = raw.filter(fmin, fmax)\n",
        "\n",
        "            # downsample\n",
        "            if freq != raw.info[\"sfreq\"]:\n",
        "                raw = raw.resample(freq)\n",
        "\n",
        "        return raw\n",
        "\n",
        "    def __call__(self, session: str) -> tuple[np.ndarray, list[str]]:\n",
        "        \"\"\" Segment the neural data around words \"\"\"\n",
        "\n",
        "        raw = self.prepare_neuro(session)\n",
        "\n",
        "        if self.apply_robust_scaler:\n",
        "            raw.load_data()\n",
        "            raw._data = RobustScaler().fit_transform(raw._data.T).T\n",
        "\n",
        "        events = pd.read_csv(self.preproc_path / \"events.csv\")\n",
        "\n",
        "        # Select the words in the relevant session\n",
        "        words = events[(events['type'] == 'Word') & (events['session'] == session)].dropna().reset_index(drop=True)\n",
        "\n",
        "        # Get word onsets in samples\n",
        "        word_onsets = np.ones((len(words), 3), dtype=int) # mne.epochs expects events of shape (n_events, 3) but we are only interested in the first column here -> set the rest to 0\n",
        "        word_onsets[:, 0] = words.start *raw.info[\"sfreq\"] # first column must contain the onset of each event (word) in samples\n",
        "\n",
        "        # Segment\n",
        "        segments = mne.Epochs(\n",
        "            raw,\n",
        "            word_onsets,\n",
        "            metadata=words,\n",
        "            event_repeated=\"drop\",\n",
        "            baseline=(self.tmin, 0),  # setting a baseline (-0.2, 0) can improve decoding results. Baselining subtracts the mean value over this window from the entire segment.\n",
        "            tmin=self.tmin,\n",
        "            tmax=self.tmax,\n",
        "            verbose=\"ERROR\"\n",
        "        )\n",
        "        del raw\n",
        "\n",
        "        # from mne to numpy\n",
        "        words = segments.metadata['text']\n",
        "        neuro_array = segments.get_data(verbose=\"ERROR\").astype(np.float32)\n",
        "        del segments\n",
        "\n",
        "        # clip segments to prevent outliers impacting regression\n",
        "        neuro_array = np.clip(neuro_array, a_min=-20, a_max=20)\n",
        "\n",
        "        n_words, n_channels, n_times = neuro_array.shape\n",
        "\n",
        "        return neuro_array, words\n",
        "\n",
        "\n",
        "class Feature(pydantic.BaseModel):\n",
        "    model_config = pydantic.ConfigDict(extra=\"forbid\")\n",
        "\n",
        "    feature: tp.Literal['zipf_frequency', 'word_embedding']\n",
        "    _model = None\n",
        "\n",
        "    infra: MapInfra = MapInfra(version=\"v1\")\n",
        "\n",
        "    def model_post_init(self, __context):\n",
        "        if self.feature == \"word_embedding\":\n",
        "            try:\n",
        "                spacy.load(\"en_core_web_lg\")  # Ensure the spaCy model is loaded\n",
        "            except OSError:\n",
        "                spacy.cli.download(\"en_core_web_lg\")  # Ensure the spaCy model is downloaded\n",
        "\n",
        "    @infra.apply(item_uid=lambda x: str(x))\n",
        "    def embed(self, words: list[str]) -> tp.Iterator[np.array]:\n",
        "        if self._model is None:\n",
        "            self._model = spacy.load(\"en_core_web_lg\")\n",
        "        for word in words:\n",
        "            yield self._model(word).vector.astype(np.float32)\n",
        "\n",
        "\n",
        "class Data(pydantic.BaseModel):\n",
        "    model_config = pydantic.ConfigDict(extra=\"forbid\")\n",
        "\n",
        "    neuro: Neuro\n",
        "    feature: Feature\n",
        "    n_sessions: int = 1\n",
        "\n",
        "    def __call__(self) -> tuple[np.ndarray, np.ndarray]:\n",
        "\n",
        "        \"\"\" concatenate neural data and feature over multiple sessions \"\"\"\n",
        "\n",
        "        # get data\n",
        "        neuro_array = []\n",
        "        words = []\n",
        "\n",
        "        print(\"Preparing neuro\")\n",
        "        sessions = [i.name.split(\"_preproc\")[0] for i in sorted(list(self.neuro.preproc_path.glob(\"*.fif\")))] # neuro files available in path\n",
        "        if self.n_sessions > len(sessions):\n",
        "            raise ValueError(f\"You requested {self.n_sessions} but there are only {len(sessions)} available.\")\n",
        "\n",
        "        for session in sessions[:self.n_sessions]:\n",
        "            session_neuro_array, session_words = self.neuro(session)\n",
        "            neuro_array.append(session_neuro_array)\n",
        "            words.extend(session_words)\n",
        "        neuro_array = np.concatenate(neuro_array, 0)\n",
        "\n",
        "        # compute embedding\n",
        "        print(\"Preparing feature\")\n",
        "        feature_array = np.vstack(list(self.feature.embed(words)))  # (n_words, n_dims)\n",
        "\n",
        "        return neuro_array, feature_array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyCxppoUQhrM"
      },
      "source": [
        "## 1. Preparing the dataloaders\n",
        "\n",
        "We start by loading the preprocessed MEG windows and word embeddings from the LibriBrain dataset, using the `Data` class defined in part 1.\n",
        "\n",
        "*Note*: For the purpose of this tutorial, we only make available (and load) a part of the entire LibriBrain dataset (3 sessions out of 92). Loading more recordings will likely improve results, though will take more time at the data preparation and model training steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXj2Mmg0QhrM"
      },
      "outputs": [],
      "source": [
        "data_config = {\n",
        "    \"neuro\": {\n",
        "        \"preproc_path\": DATA_DIR,\n",
        "        \"tmin\": -0.2,\n",
        "        \"tmax\": 1.8,\n",
        "        \"apply_robust_scaler\": True,\n",
        "    },\n",
        "    \"feature\": {\n",
        "        \"feature\": \"word_embedding\",\n",
        "        \"infra\": {\n",
        "            \"folder\": CACHE_DIR,\n",
        "        },\n",
        "    },\n",
        "    \"n_sessions\": 4,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxWITGyLQhrM"
      },
      "outputs": [],
      "source": [
        "# Get neuro and word data (~75 s to run)\n",
        "data = Data(**data_config)\n",
        "X, y = data()\n",
        "\n",
        "print(f\"\\nData shape\\nX: {X.shape}\\ny: {y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6FKNrraQhrM"
      },
      "source": [
        "Now that we have our windowed data, we can split it into the different sets that are needed for modeling:  \n",
        "(1) the training set is used to learn the parameters of our deep learning model,  \n",
        "(2) the validation set is used to monitor the training process and decide when to stop it, and  \n",
        "(3) the test set is used to provide an estimate of the generalization performance of our model.\n",
        "\n",
        "Here, we use the last 10% of windows for testing, and split the remaining 90% of windows into training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXwrbqYcKOhh"
      },
      "outputs": [],
      "source": [
        "# Prepare dataloaders\n",
        "\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "valid_split_ratio = 0.1\n",
        "test_split_ratio = 0.1\n",
        "random_state = 42\n",
        "\n",
        "# Split into train/valid/test\n",
        "inds = np.arange(X.shape[0])\n",
        "inds_train, inds_test = train_test_split(\n",
        "    inds, shuffle=False, test_size=test_split_ratio,\n",
        ")\n",
        "inds_train, inds_valid  = train_test_split(\n",
        "    inds_train,\n",
        "    shuffle=True,\n",
        "    test_size=valid_split_ratio / (1 - test_split_ratio),\n",
        "    random_state=random_state,\n",
        ")\n",
        "\n",
        "X_train, X_valid, X_test = X[inds_train], X[inds_valid], X[inds_test]\n",
        "y_train, y_valid, y_test = y[inds_train], y[inds_valid], y[inds_test]\n",
        "del X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdkJQiTeQhrM"
      },
      "source": [
        "We standardize the word embeddings to have zero mean and unit variance, which is a common practice in machine learning to ensure that all features contribute equally to the model training and to stabilize training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4-X9IVyQhrM"
      },
      "outputs": [],
      "source": [
        "# Normalize targets\n",
        "target_scaler = StandardScaler()\n",
        "y_train = target_scaler.fit_transform(y_train)\n",
        "y_valid = target_scaler.transform(y_valid)\n",
        "y_test = target_scaler.transform(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe7ONgC6QhrM"
      },
      "source": [
        "Finally, we create pytorch `DataLoader`s, which will be used to feed the data to the model during training and evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTIL4-0NQhrN"
      },
      "outputs": [],
      "source": [
        "# Create datasets and dataloaders\n",
        "batch_size = 128\n",
        "num_workers = 2\n",
        "\n",
        "train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "valid_dataset = TensorDataset(torch.tensor(X_valid), torch.tensor(y_valid))\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "test_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "print(\"Number of examples in each split\")\n",
        "print(f\"Train:\\t{len(train_dataset)}\")\n",
        "print(f\"Valid:\\t{len(valid_dataset)}\")\n",
        "print(f\"Test:\\t{len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3JjNTe0QhrN"
      },
      "source": [
        "## 2. Building the deep learning model\n",
        "\n",
        "In this section, we will define our deep learning architecture.\n",
        "\n",
        "We use the residual dilated convolutional neural network of [Défossez et al. (2023)](https://www.nature.com/articles/s42256-023-00714-5). It performed well on multiple other M/EEG decoding tasks, e.g. [image decoding ](https://arxiv.org/abs/2310.19812), [word decoding](https://arxiv.org/abs/2412.17829) and [typing decoding](https://arxiv.org/abs/2502.17480).\n",
        "\n",
        "Following the formulation in part 1, we have:\n",
        "\n",
        "$\\widehat{y}_i = f_{\\Theta}(X_i) $\n",
        "\n",
        "where\n",
        "- $X_i$: an MEG segment, with shape `(n_channels, n_times)`;\n",
        "- $\\widehat{y}$: the predicted word embedding, with shape `(n_word_dims,)`;\n",
        "- $\\Theta$: the parameters of our convolutional network, to be tuned during training.\n",
        "\n",
        "![model](https://github.com/lucyzmf/NeuralDecoding-CCN2025/blob/main/images/model_architecture.png?raw=1)\n",
        "*Architecture of our simplified brain module.*\n",
        "\n",
        "*Note*: The following implementation is a simplified version of the [original](https://github.com/facebookresearch/brainmagick/blob/main/bm/models/simpleconv.py#L22) in which we drop less critical features and hardcode some hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XTeMSwYEQhrN"
      },
      "outputs": [],
      "source": [
        "#@title ▶️ Run this first to define the model class\n",
        "\n",
        "import lightning as pl\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class ConvSequence(nn.Module):\n",
        "    \"\"\"Sequence of residual, dilated convolutional layers with GLU activation.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: list[int],\n",
        "        kernel_size: int = 4,\n",
        "        dilation_growth: int = 2,\n",
        "        dilation_period: int = 5,\n",
        "        glu: int = 2,\n",
        "        glu_context: int = 1,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        if dilation_growth > 1:\n",
        "            assert kernel_size % 2 != 0, \"Supports only odd kernel with dilation for now\"\n",
        "\n",
        "        self.sequence = nn.ModuleList()\n",
        "        self.glus = nn.ModuleList()\n",
        "\n",
        "        dilation = 1\n",
        "        for k, (chin, chout) in enumerate(zip(channels[:-1], channels[1:])):\n",
        "            layers: list[nn.Module] = []\n",
        "\n",
        "            # conv layer\n",
        "            if dilation_period and (k % dilation_period) == 0:\n",
        "                dilation = 1\n",
        "\n",
        "            layers.extend([\n",
        "                nn.Conv1d(\n",
        "                    chin,\n",
        "                    chout,\n",
        "                    kernel_size=kernel_size,\n",
        "                    stride=1,\n",
        "                    padding=kernel_size // 2 * dilation,\n",
        "                    dilation=dilation,\n",
        "                    groups=1,\n",
        "                ),\n",
        "                nn.BatchNorm1d(num_features=chout),\n",
        "                nn.GELU(),\n",
        "            ])\n",
        "            dilation *= dilation_growth\n",
        "\n",
        "            self.sequence.append(nn.Sequential(*layers))\n",
        "            if (k + 1) % glu == 0:\n",
        "                self.glus.append(\n",
        "                    nn.Sequential(\n",
        "                        nn.Conv1d(chout, 2 * chout, 1 + 2 * glu_context, padding=glu_context),\n",
        "                        nn.GLU(dim=1),\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                self.glus.append(None)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        for ind, module in enumerate(self.sequence):\n",
        "            x = x + module(x)\n",
        "            if self.glus[ind] is not None:\n",
        "                x = self.glus[ind](x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SimpleConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        n_hidden_dims: int = 320,\n",
        "        n_conv_blocks: int = 2,\n",
        "        kernel_size: int = 3,\n",
        "        growth: float = 1.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert kernel_size % 2 == 1, \"For padding to work, this must be verified\"\n",
        "        self.input_linear = nn.Conv1d(in_channels, n_hidden_dims, 1)\n",
        "\n",
        "        # Build sequence of convolutional layers\n",
        "        sizes = [n_hidden_dims] + [\n",
        "            int(round(n_hidden_dims * growth**k)) for k in range(n_conv_blocks * 2)\n",
        "        ]\n",
        "        self.encoder = ConvSequence(\n",
        "            sizes, kernel_size=kernel_size,\n",
        "        )\n",
        "\n",
        "        # Temporal aggregation\n",
        "        self.time_aggregation = nn.LazyLinear(1)\n",
        "        self.output_linear = nn.Linear(n_hidden_dims, out_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # B, C, T = x.shape\n",
        "        x = self.input_linear(x)\n",
        "        x = self.encoder(x)\n",
        "        x = self.time_aggregation(x).squeeze(-1)  # (B, F, 1) -> (B, F)\n",
        "        x = self.output_linear(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjCc71v0yiDE"
      },
      "source": [
        "In the following, what should we set `n_hidden_dims` and `n_conv_blocks` to? Run the following two cells to see the impact on the architecture (and the number of parameters):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dag_0ItKXRfZ"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "\n",
        "n_hidden_dims = 1  # TODO: Set to something else?\n",
        "n_conv_blocks = 1  # TODO: Set to something else?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_C2fB3vXUP3"
      },
      "outputs": [],
      "source": [
        "#@title Answer - suggested hyperparameters\n",
        "n_hidden_dims = 320\n",
        "n_conv_blocks = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9C4zgcDyQhrN"
      },
      "outputs": [],
      "source": [
        "pl.seed_everything(42)\n",
        "\n",
        "model = SimpleConv(\n",
        "    in_channels=X_train.shape[1],\n",
        "    out_channels=y_train.shape[1],\n",
        "    n_hidden_dims=n_hidden_dims,  # Number of hidden dimensions\n",
        "    n_conv_blocks=n_conv_blocks,  # Number of convolutional layers\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6AKlB_EQhrN"
      },
      "source": [
        "Let's print a summary of the model to see its architecture and number of parameters $\\Theta$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hel0NXhoQhrN"
      },
      "outputs": [],
      "source": [
        "# Print model summary\n",
        "from torchinfo import summary\n",
        "\n",
        "batch = next(iter(train_dataloader))\n",
        "input_data = {\"x\": batch[0][[0]]}\n",
        "summary(model, input_data=input_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6QM6MJjzerv"
      },
      "source": [
        "*Note*: there are a lot of other architectural hyperparameters that could be modified and that might work better than this default configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05navVCcQhrN"
      },
      "source": [
        "## 3. Designing the training loop\n",
        "\n",
        "Two critical elements in the training process are the *optimizer* and the *loss function*.\n",
        "\n",
        "The **optimizer** implements the parameter update procedure. Here, we use `AdamW`, a popular adaptive gradient descent optimizer for deep neural networks.\n",
        "\n",
        "The **loss function** is used to measure how well the neural network performs on an example. For simplicity, here we use the *mean squared error loss*:\n",
        "\n",
        "$\\text{MSE} = \\frac{1}{F} \\sum_{i=1}^F (y_i - \\widehat{y}_i)^2$\n",
        "\n",
        "Here, we use [Pytorch Lightning](https://lightning.ai/docs/pytorch) to simplify the creation of these different elements. Pytorch Lightning is a wrapper around `torch` that provides a framework and utilities to simplify the training and evaluation of deep learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acB9LWnkQhrN"
      },
      "outputs": [],
      "source": [
        "class BrainModule(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        loss_fn: nn.Module,\n",
        "        learning_rate: float,\n",
        "        weight_decay: float,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.loss_fn = loss_fn\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "    def configure_optimizers(self) -> nn.Module:\n",
        "        return torch.optim.AdamW(\n",
        "            self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "    def predict_step(\n",
        "        self, batch, batch_idx: int, dataloader_idx: int = 0,\n",
        "    ) -> torch.Tensor:\n",
        "        return self.forward(batch[0])\n",
        "\n",
        "    def _run_step(self, batch) -> torch.Tensor:\n",
        "        x, y_true = batch\n",
        "        y_pred = self.forward(x)\n",
        "        return self.loss_fn(y_pred, y_true)\n",
        "\n",
        "    def training_step(\n",
        "        self, batch, batch_idx: int,\n",
        "    ) -> torch.Tensor:\n",
        "        loss = self._run_step(batch)\n",
        "        self.log(\"train_loss\", loss, on_epoch=True, on_step=False, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch: tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> None:\n",
        "        loss = self._run_step(batch)\n",
        "        self.log(\"valid_loss\", loss, on_epoch=True, on_step=False, prog_bar=True)\n",
        "\n",
        "    def test_step(self, batch: tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> None:\n",
        "        loss = self._run_step(batch)\n",
        "        self.log(\"test_loss\", loss, on_epoch=True, on_step=False, prog_bar=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhfDxmyiQhrN"
      },
      "source": [
        "We define our lightning module with some default parameters (we will explore changing these later):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWvne5sRQhrN"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "module = BrainModule(\n",
        "    model=copy.deepcopy(model),\n",
        "    loss_fn=nn.MSELoss(),\n",
        "    learning_rate=3e-4,\n",
        "    weight_decay=0.01,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaH6yZJOQhrN"
      },
      "source": [
        "Next, we create a `Trainer`, which will handle the training loop.\n",
        "\n",
        "We include two callbacks, one for *early stopping*, and one for *model checkpointing*. The `patience` hyperparameter controls how many epochs we will wait for before stopping the training process if there is no improvement on the validation set. The best model based on validation set performance will be saved, and then reloaded for final evaluation on the test set.\n",
        "\n",
        "Note: The maxmium number of training epochs (or \"passes\" through the training set) is set with `max_epochs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7ZV2Vl8QhrN"
      },
      "outputs": [],
      "source": [
        "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from lightning.pytorch.loggers import CSVLogger\n",
        "\n",
        "logger = CSVLogger(RESULTS_DIR / \"logs\", name=\"word_decoding\")\n",
        "\n",
        "# Create callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor=\"valid_loss\",\n",
        "        min_delta=0.00,\n",
        "        patience=3,\n",
        "        mode=\"min\",\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        filename=\"best\",\n",
        "        monitor=\"valid_loss\",\n",
        "        mode=\"min\",\n",
        "        save_top_k=1,\n",
        "    ),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=6,\n",
        "    accelerator=\"auto\",\n",
        "    callbacks=callbacks,\n",
        "    logger=logger,\n",
        "    num_sanity_val_steps=0,  # Turn off sanity checking of validation dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y1E_voMQhrN"
      },
      "source": [
        "## 4. Training the model\n",
        "\n",
        "We can now launch our training loop!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2eEbAmZQhrO"
      },
      "outputs": [],
      "source": [
        "# Takes about 25 s per epoch\n",
        "trainer.fit(\n",
        "    model=module,\n",
        "    train_dataloaders=train_dataloader,\n",
        "    val_dataloaders=valid_dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMEf-NXcQhrO"
      },
      "source": [
        "Next, we visualize the results of our training.\n",
        "\n",
        "The *learning curves* show how the training and validation losses evolved across training epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMm7Y5uqQhrO"
      },
      "outputs": [],
      "source": [
        "# Load the logged metrics\n",
        "log_output_df = pd.read_csv(Path(logger.log_dir) / \"metrics.csv\")\n",
        "log_output_df = log_output_df.melt(\n",
        "    id_vars=[\"epoch\"],\n",
        "    value_vars=[\"train_loss\", \"valid_loss\"],\n",
        "    var_name=\"split\",\n",
        "    value_name=\"loss\",\n",
        ").dropna(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJU_RKaHQhrO"
      },
      "outputs": [],
      "source": [
        "# Plot them\n",
        "import seaborn as sns\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "sns.lineplot(\n",
        "    data=log_output_df,\n",
        "    x=\"epoch\",\n",
        "    y=\"loss\",\n",
        "    hue=\"split\",\n",
        "    marker=\"o\",\n",
        "    markersize=5,\n",
        "    linewidth=1,\n",
        "    ax=ax,\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV-YEfm0QhrO"
      },
      "source": [
        "Generally, we expect the training loss to steadily decrease from epoch to epoch, and the validation loss to initially decrease, and then eventually start increasing (i.e. when the model enters the \"overfitting\" regime).\n",
        "\n",
        "*Question*: Can you mitigate early overfitting? Here are some ideas to get you started:\n",
        "* Use stronger weight decay\n",
        "* Enable dropout in the model\n",
        "* Include more data (watch out for RAM usage if you're on colab!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT9Ssp6EQhrO"
      },
      "source": [
        "## 5. Evaluating test performance\n",
        "\n",
        "As we did in part 1, we can also measure the performance of our model on a test set which was not seen during training (i.e. \"held-out\"). This gives us an estimate of the generalization performance of our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0XdAPJnQhrO"
      },
      "outputs": [],
      "source": [
        "y_true = torch.concat([batch[1] for batch in test_dataloader], dim=0)\n",
        "y_pred = torch.concat(trainer.predict(dataloaders=test_dataloader), dim=0)\n",
        "\n",
        "assert y_pred.shape == y_true.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpTduPEaQhrO"
      },
      "outputs": [],
      "source": [
        "# Compute Pearson correlation\n",
        "corr = pearsonr(y_true, y_pred, axis=0).statistic\n",
        "print(f\"Mean Pearson R: {corr.mean():0.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc_gyvvcQhrO"
      },
      "source": [
        "As we can see, most dimensions of the word embeddings can be predicted with a positive correlation.\n",
        "\n",
        "How does this compare to the results you obtained in Part 1?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrd09u1CQhrS"
      },
      "source": [
        "## 6. Leveraging `exca` for large-scale experimentation\n",
        "\n",
        "Finally, as we did in part 1, we can leverage `exca` to structure the entire pipeline into a single configurable class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYR2moZiQhrS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from lightning.pytorch.loggers import CSVLogger\n",
        "\n",
        "\n",
        "class DeepDecodingExperiment(pydantic.BaseModel):\n",
        "    data: Data\n",
        "\n",
        "    # Splits\n",
        "    valid_split_ratio: float = 0.1\n",
        "    test_split_ratio: float = 0.1\n",
        "    split_random_state: int | None = 42\n",
        "\n",
        "    # Dataloaders\n",
        "    batch_size: int = 128\n",
        "    num_workers: int = 10\n",
        "\n",
        "    # Model\n",
        "    seed: int | None = None\n",
        "\n",
        "    # Optimization\n",
        "    loss_name: tp.Literal[\"mse\", \"clip\"] = \"mse\"\n",
        "    learning_rate: float = 3e-4\n",
        "    weight_decay: float = 0.01\n",
        "    n_epochs: int = 10\n",
        "    patience: int = 3\n",
        "\n",
        "    # exca\n",
        "    infra: TaskInfra = TaskInfra()\n",
        "\n",
        "    def _prepare_loaders(self, X: np.ndarray, y: np.ndarray) -> list[DataLoader]:\n",
        "        \"\"\"Prepare dataloaders.\"\"\"\n",
        "\n",
        "        # Split into train/valid/test\n",
        "        X_train, X_test, y_train, y_test  = train_test_split(\n",
        "            X,\n",
        "            y,\n",
        "            shuffle=False,\n",
        "            test_size=self.test_split_ratio,\n",
        "            # random_state=self.split_random_state,\n",
        "        )\n",
        "        X_train, X_valid, y_train, y_valid  = train_test_split(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            shuffle=True,\n",
        "            test_size=self.valid_split_ratio / (1 - self.test_split_ratio),\n",
        "            # random_state=self.split_random_state,\n",
        "        )\n",
        "\n",
        "        # Normalize targets\n",
        "        target_scaler = StandardScaler()\n",
        "        y_train = target_scaler.fit_transform(y_train)\n",
        "        y_valid = target_scaler.transform(y_valid)\n",
        "        y_test = target_scaler.transform(y_test)\n",
        "\n",
        "        # Create datasets and dataloaders\n",
        "        train_dataset = TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_train).float())\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
        "\n",
        "        valid_dataset = TensorDataset(torch.tensor(X_valid).float(), torch.tensor(y_valid).float())\n",
        "        valid_dataloader = DataLoader(valid_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
        "\n",
        "        test_dataset = TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test).float())\n",
        "        test_dataloader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
        "\n",
        "        print(\"Number of examples in each split\")\n",
        "        print(f\"Train:\\t{len(train_dataset)}\")\n",
        "        print(f\"Valid:\\t{len(valid_dataset)}\")\n",
        "        print(f\"Test:\\t{len(test_dataset)}\")\n",
        "\n",
        "        return train_dataloader, valid_dataloader, test_dataloader\n",
        "\n",
        "    def _prepare_model(self, in_channels: int, out_channels: int) -> SimpleConv:\n",
        "        return SimpleConv(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            n_hidden_dims=320,\n",
        "            n_conv_blocks=4,\n",
        "        )\n",
        "\n",
        "    def _prepare_trainer(self) -> pl.Trainer:\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor=\"valid_loss\",\n",
        "                min_delta=0.00,\n",
        "                patience=self.patience,\n",
        "                mode=\"min\",\n",
        "            ),\n",
        "            ModelCheckpoint(\n",
        "                filename=\"best\",\n",
        "                monitor=\"valid_loss\",\n",
        "                mode=\"min\",\n",
        "                save_top_k=1,\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        logger = CSVLogger(self.infra.uid_folder() / \"logs\", name=\"word_decoding\")\n",
        "\n",
        "        return pl.Trainer(\n",
        "            max_epochs=self.n_epochs,\n",
        "            accelerator=\"auto\",\n",
        "            callbacks=callbacks,\n",
        "            num_sanity_val_steps=0,  # Turn off sanity checking of validation dataloader\n",
        "            logger=logger,\n",
        "        )\n",
        "\n",
        "    def _evaluate_corr(self, trainer: pl.Trainer, test_loader: DataLoader) -> np.ndarray:\n",
        "        y_true = torch.concat([batch[1] for batch in test_loader], dim=0)\n",
        "        y_pred = torch.concat(trainer.predict(dataloaders=test_loader), dim=0)\n",
        "        return pearsonr(y_true, y_pred, axis=0).statistic\n",
        "\n",
        "    @infra.apply\n",
        "    def run(self) -> np.ndarray:\n",
        "        \"\"\"Fit and evaluate a deep decoding model.\"\"\"\n",
        "\n",
        "        print(f\"Decoding {self.data.feature.feature} from {self.data.n_sessions} sessions\")\n",
        "        X, y = self.data()\n",
        "        train_loader, valid_loader, test_loader = self._prepare_loaders(X, y)\n",
        "\n",
        "        # Prepare model\n",
        "        pl.seed_everything(self.seed)\n",
        "        model = self._prepare_model(X.shape[1], y.shape[1])\n",
        "\n",
        "        # Prepare pytorch lightning module\n",
        "        loss_fn = nn.MSELoss() if self.loss_name == \"mse\" else ClipLoss()\n",
        "        module = BrainModule(\n",
        "            model=model,\n",
        "            loss_fn=loss_fn,\n",
        "            learning_rate=self.learning_rate,\n",
        "            weight_decay=self.weight_decay,\n",
        "        )\n",
        "\n",
        "        # Prepare trainer\n",
        "        trainer = self._prepare_trainer()\n",
        "\n",
        "        print(\"Fitting model\")\n",
        "        trainer.fit(\n",
        "            model=module,\n",
        "            train_dataloaders=train_loader,\n",
        "            val_dataloaders=valid_loader\n",
        "        )\n",
        "\n",
        "        # Predict and evaluate performance\n",
        "        corr = self._evaluate_corr(trainer, test_loader)\n",
        "        print(f\"Test mean Pearson R: {corr.mean():0.3f}\")\n",
        "\n",
        "        return corr\n",
        "\n",
        "\n",
        "\n",
        "# --- BONUS - For contrastive learning with CLIP loss ---\n",
        "\n",
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "class ClipLoss(nn.Module):\n",
        "    \"\"\"CLIP constrastive loss [1]_ with configuration from [2]_.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Radford, Alec, et al. \"Learning transferable visual models from natural language\n",
        "        supervision.\" International conference on machine learning. PMLR, 2021.\n",
        "    .. [2] Défossez, Alexandre, et al. \"Decoding speech perception from non-invasive brain\n",
        "        recordings.\" Nature Machine Intelligence (2023): 1-11.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def _compute_similarity(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "        inv_norms = 1 / (y.norm(dim=(1), p=2) + 1e-15)\n",
        "        return torch.einsum(\"bc,oc,o->bo\", x, y, inv_norms)\n",
        "\n",
        "    # XXX Change to y_pred, y_true\n",
        "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Warning: y_pred and y_true are not necessarily symmetrical.\n",
        "\n",
        "        If y_pred of shape [B, C] and y_true of shape [B', C] with B'>=B, the first B samples\n",
        "        of y_true are targets, while the remaining B'-B samples of y_true are only used as\n",
        "        negatives.\n",
        "        \"\"\"\n",
        "        scores = self._compute_similarity(y_pred, y_true)\n",
        "        target = torch.arange(len(scores), device=y_pred.device)\n",
        "\n",
        "        return cross_entropy(scores, target, reduction=\"mean\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unTARU7UQhrS"
      },
      "source": [
        "Our entire experiment can now be configured using a dictionary (let's make a version that will run quickly to start with):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_jXNPHRQhrS"
      },
      "outputs": [],
      "source": [
        "exp_config = {\n",
        "  \"data\": {\n",
        "      \"neuro\": {\n",
        "          \"preproc_path\": DATA_DIR,\n",
        "          \"tmin\": -0.2,\n",
        "          \"tmax\": 1.8 - 1/80,\n",
        "          \"apply_robust_scaler\": True,\n",
        "      },\n",
        "      \"feature\": {\n",
        "          \"feature\": \"word_embedding\",\n",
        "          \"infra\": {\n",
        "              \"folder\": CACHE_DIR,\n",
        "          },\n",
        "      },\n",
        "      \"n_sessions\": 2,\n",
        "  },\n",
        "  # Splitting\n",
        "  \"valid_split_ratio\": 0.1,\n",
        "  \"test_split_ratio\": 0.1,\n",
        "  \"split_random_state\": 42,\n",
        "  # Dataloaders\n",
        "  \"batch_size\": 128,\n",
        "  \"num_workers\": 10,\n",
        "  # Optimization\n",
        "  \"loss_name\": \"mse\",\n",
        "  \"learning_rate\": 3e-4,\n",
        "  \"weight_decay\": 0.01,\n",
        "  \"n_epochs\": 1,\n",
        "  \"patience\": 1,\n",
        "  # Exca\n",
        "  \"infra\": {\n",
        "      \"folder\": RESULTS_DIR,\n",
        "  },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN8F09FEQhrS"
      },
      "outputs": [],
      "source": [
        "exp = DeepDecodingExperiment(**exp_config)\n",
        "\n",
        "# Uncomment to run the experiment (will take some time and memory):\n",
        "# corr = exp.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nhBkVzwQhrS"
      },
      "source": [
        "Note: using `exca`, we can easily launch multiple versions of our experiment in parallel with a [SLURM array](https://slurm.schedmd.com/job_array.html).\n",
        "\n",
        "E.g., we could test the impact of different loss functions, batch sizes, learning rates and random seeds like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irU3-BpgQhrS"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "from itertools import product\n",
        "from tqdm import tqdm\n",
        "\n",
        "loss_names = [\"mse\", \"clip\"]\n",
        "batch_sizes = [32, 64, 128]\n",
        "learning_rates = [3e-5, 3e-4, 3e-3]\n",
        "seeds = [33, 34, 35]\n",
        "\n",
        "task = DeepDecodingExperiment(**exp_config)\n",
        "with task.infra.job_array() as tasks:\n",
        "    for (\n",
        "        loss_name,\n",
        "        batch_size,\n",
        "        learning_rate,\n",
        "        seed\n",
        "    ) in tqdm(list(product(\n",
        "        loss_names,\n",
        "        batch_sizes,\n",
        "        learning_rates,\n",
        "        seeds,\n",
        "    )), \"Preparing tasks\"):\n",
        "        updated_config = deepcopy(exp_config)\n",
        "        updated_config[\"loss_name\"] = loss_name\n",
        "        updated_config[\"batch_size\"] = batch_size\n",
        "        updated_config[\"learning_rate\"] = learning_rate\n",
        "        updated_config[\"seed\"] = seed\n",
        "\n",
        "        # Uncomment these lines on a SLURM cluster to launch all jobs as a SLURM array:\n",
        "        # task_ = DeepDecodingExperiment(**updated_config)\n",
        "        # tasks.append(task_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PiC09GZQhrS"
      },
      "source": [
        "## 7. Going further\n",
        "\n",
        "This is it for part 3 of this tutorial! 🎉\n",
        "\n",
        "🔧 To improve the *flexibility of our pipeline*, next steps can include:\n",
        "- Further exposing components as configurable Pydantic objects (e.g. deep learning architecture, loss functions, etc.)\n",
        "- Monitoring performance metrics during training using [TorchMetrics](https://lightning.ai/docs/torchmetrics/)\n",
        "- Logging results to a live visualization platform, e.g. [Weights & Biases](https://wandb.ai/) or [Tensorboard](https://www.tensorflow.org/tensorboard)\n",
        "\n",
        "📈 To improve *performance*, next steps can include:\n",
        "- Including more recordings into our training dataset\n",
        "- Improving the architecture (deeper/wider, handling channel positions, word-level modelling, etc.)\n",
        "- Optimizing the hyperparameters (learning rate, batch size, etc.).\n",
        "\n",
        "Have fun!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "brainai_v2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}